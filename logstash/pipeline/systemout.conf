input{
    file {
            path => "/usr/share/logstash/logs/systemout.log" #Path de donde cogerá los logs
            #exclude => "*.json" #Excluir archivos que seguirán el patrón siguiente
            start_position => "beginning" #Comenzará a leer desde el principio, también podemos poner 'end'
            sincedb_path => "/dev/null" # Puntero para guardar la posicion donde se ha quedado
          #  codec => multiline { #Habrá más de una linea para un evento (java.lang.error)
          #    pattern => "^www.*" #Empeza por www. y continuar por lo que sea
          #    negate => "true" #Los que no coincidan con el patron
          #    what => "previous" #Los que no encajen lo asignarán al evento anterior 
            }
    
      #   Podríamos tener varios Pipelines con diferentes puertos escuchado
      #   de manera que tendríamos varios Beats enviando información diferente
      #   y siendo recogidos por cada logstash
}

filter {
    #Obtener solamente el texto que empiece por "{.*$" para obtener
    #todo el texto con formato json y guardarlo en logs_systemout
    #Para luego obtener ese recurso que no es más que el texto obtenido
    #del message y transformarlo en json
    grok {
        match => {
            "message" => ["%{JSON:logs_systemout}"]
        }
        pattern_definitions => {
            "JSON" => "{.*$"
        }
    }

    json {
        source => "logs_systemout"
    }

    #Borrar todos los documentos que contenga el tags _grokparsefailure
    if "_grokparsefailure" in [tags] {
        drop{}
    }

    #Eliminar los campos message, system, version
    mutate {
        remove_field => ["@version", "system", "logs_systemout", "message"]
    }
}

output{
    stdout{
        codec => rubydebug
        }
    elasticsearch{
        hosts => "elasticsearch:9200"
        index => "systemout2-%{+YYYY.MM.dd}"
    }
}